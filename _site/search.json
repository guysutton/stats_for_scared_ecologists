[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogposts",
    "section": "",
    "text": "General Linear Model (GLM): Gaussian GLM\n\n\n\n\n\n\n\nR\n\n\ntidyverse\n\n\nGLM\n\n\nggplot2\n\n\nModel diagnostics\n\n\nDHARMa\n\n\nHypothesis testing\n\n\n\n\nA gentle introduction\n\n\n\n\n\n\nAug 4, 2021\n\n\nGuy. F. Sutton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2021-08-04-gaussian-general-linear-model-glm/gaussian-general-linear-model-glm.html",
    "href": "posts/2021-08-04-gaussian-general-linear-model-glm/gaussian-general-linear-model-glm.html",
    "title": "General Linear Model (GLM): Gaussian GLM",
    "section": "",
    "text": "Exploratory data analysis\nThe first step in our statistical analysis should familiarising ourselves with the data. To do this, you should be performing exploratory data analysis (EDA).\n\n(i) Check the data corresponds to the experimental design\nThe first thing I usually do is check that the number of replicates per treatment is in line with the experimental design. In other words, we sampled 30 mussels per site, so let‚Äôs just check that each site has 30 mussel body mass measurements.\n\n# Start with the data frame\ndata %>%\n  # Calculate the number of replicates per 'site' \n  dplyr::count(site)\n\n  site  n\n1    A 30\n2    B 30\n\n\nAs expected, there are 30 rows of data per site. Good start!\n\n\n(ii) Visualise the data\nThe next step is to plot your data. You should be noting aspects such as:\n\nIs there a visual difference in means or the distributions of values in the different groups?\nIs there more variation in one group vs another group?\n\n\n# Start with the data frame\ndata %>% \n  ggplot(data = ., aes(x = site,\n                       y = body_mass)) +\n  geom_boxplot()\n\n\n\n\nIn this case, it looks like the median body mass (indicated by the bold black line) of mussels collected at site A is lower than than at site B). The variance (i.e.¬†spread of body mass values along the y-axis) appears to be relatively similar between the two sites.\n\n\n\nFitting the model\nFitting a GLM is relatively simple in R. All we need to do is tell it what is our response variable (the response variable is the measurement we are interested in). Here, the response variable is (body_mass). We then specify our predictor variables to the right-hand side of this weird ~ (tilde) symbol. Our predictor variables are things we have recorded that we believe could be affecting the response variable. Here, our predictor variable was site. We need to tell R where these data are stored (data), and that we want a gaussian GLM (family = gaussian()).\n\nmod1 <- glm(body_mass ~ site, \n            data = data,\n            family = gaussian(link = \"identity\"))\n\n\n\nEvaluating model fit\nBefore we look at the results from our model, we must first check whether the GLM that we fit was an appropriate choice for our data. We do that by looking at model diagnostics. Model diagnostics rely heavily on calculating and visualising residuals. For example, let‚Äôs assume we had fit a model looking at whether body length is a predictor of body mass below. The residuals are the filled black circles, indicating the deviation between our observed data value and its expected value under the given model (indicating by the unfilled circle). Simply, residuals are the observed data value minus its expected value from a model. GLM‚Äôs make assumptions about the distribution of the residuals which we are going to unpack below.\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nIn my opinion, the easiest and most informative way perform model diagnostics check in R is using the amazing package DHARMa. Below, we are going to plot two different graphs to evaluate whether our choice of model was okay.\nBefore we make any plots, we have to get DHARMa to produce residuals for us to use in our plots. We will feed these residuals into the different plotting functions.\n\nsimulated_resids <- DHARMa::simulateResiduals(fittedModel = mod1, \n                                              plot = F)\n\n\n(i) Plot #1: QQPlot\nThe QQ plot tells us whether our data conforms to the distribution we specified in the family argument in the GLM call above (remember: we said family = gaussian). If our GLM is a good fit to the data, the white triangles will fall approximately on the red 1:1 line and the KS test P-value will be greater than 0.05. The Kolmorogorov-Smirnoff test (KS test) is a formal statistical test to evaluate whether our data follow the distribution we specified in the family argument in the GLM call above.\n\nDHARMa::plotQQunif(simulated_resids)\n\n\n\n\nHere, we can see two important things. Firstly, the white triangles fall approximately along the red 1:1 line, indicating that our residuals (the white triangles) approximate the data distribution we specified in family = .... Secondly, the Kolmorogorov-Smirnoff test (KS test) provides further support that the residuals from our model were not significantly different from the the data distribution we specified in family = .... Taken together, the QQplot shows us that our residuals are approximately normally distributed. Great!\n\n\n(ii) Plot #2: Residuals vs Predicted Plot\nThis plots allows us to evaluate whether our data demonstrates heteroscedasticity. This is just a fancy way to say that the variance in the data is systematically dependent on some variable in the model. If you remember back to undergraduate statistics courses, linear models typically assume that the data displays equal variances across groups or numeric variables. When our predictor variables are categorical, such as in the current model (remember site was our predictor variable with two levels, siteA and siteB), this plot will show boxplots with each level (e.g.¬†each site) getting its own boxplot.\nIf our GLM was a good fit, we would like the boxplots to be centered between y = 0.25 to y = 0.75, with the bold black line falling approximately on the y = 0.5 line. DHARMa has recently introduced a nice function which automatically produces a formal statistical test for within-group uniformity and between-group homogeneity of variances. Ultimately, we want both of these tests to return n.s. meaning a non-significant result.\n\nDHARMa::plotResiduals(simulated_resids)\n\n\n\n\nThe plot shows us that there aren‚Äôt major concerns over unequal variances. While we would like to see the left-hand boxplox grey-shaded area range from y = 0.25 to 0.75 (it ranges from y = 0.33ish to 0.80), both the test for uniformity and unequal variances test were n.s.. In later posts, we will unpack model diagnostics in much greater detail. Stay tuned.\n\n\n\nStatistical inference\nNow to the bit of the analysis that most ecologists are most interested in (at least to appease their reviewers: assessing statistical significance and calculating p-values). Here, we perform statistical inference, which basically means we are going to evaluate ‚Äúwhich [model] coefficients are non-zero beyond a reasonable doubt, implying meaningful associations between covariates and the response?‚Äù (Tredennick et al. 2021).\nTo do this, we will use a Likelihood Ratio Test (LRT). When we only have 1 predictor variable (here: site), we can calculate p-values using type I sum-of-squares (SOS). SOS‚Äôs are just different ways that we ask R to calculate p-values.\nPLEASE DO NOT USE SUMMARY() - THIS WILL PRODUCE THE WRONG P-VALUES WHEN YOU HAVE MORE THAN 1 PREDICTOR VARIABLE\n\n# Perform LRT with type I sum-of-squares \nanova(mod1,\n      test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: body_mass\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                    59     8481.1              \nsite  1     1498        58     6983.1 0.0004198 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, the LRT tells us that mussel body_mass was statistically significantly different between sites (\\(\\chi\\)2 = 1498, d.f. = 1, P < 0.001).\n\n\nMake a figure\nNow we are going to make a figure to summarise our findings and that you can include in your thesis or paper.\n\n# Calculate mean +- standard error\ndata %>%\n  ggplot(data = ., aes(x = site,\n                       y = body_mass,\n                       fill = site)) +\n  geom_boxplot() +\n  # Fill the boxes by site\n  scale_fill_grey(start = 0.6) +\n  # Add significance letters\n  scale_x_discrete(\"Site \",\n                   labels = c(\"A\", \"B\")) +\n  annotate(\"text\", x = 1, y = 40, label = \"a\") +\n  annotate(\"text\", x = 2, y = 60, label = \"b\") +\n  # Change axis labels\n  labs(y = \"Mussel body mass (mg)\")\n\n\n\n\n\n\n\n\n\nConclusions\nThere we go!!! You are now equipped to run your own GLM. Please let me know if there are any improvements or what topics you would like me to cover going forward.\n\n\n\n\n\nReferences\n\nTredennick, Andrew T, Giles Hooker, Stephen P Ellner, and Peter B Adler. 2021. ‚ÄúA Practical Guide to Selecting Models for Exploration, Inference, and Prediction in Ecology.‚Äù Ecology 102 (6): e03336.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{f.sutton2021,\n  author = {Guy. F. Sutton and Guy F. Sutton},\n  title = {General {Linear} {Model} {(GLM):} {Gaussian} {GLM}},\n  date = {08/04/2021},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGuy. F. Sutton, and Guy F. Sutton. 8AD‚Äì4AD. ‚ÄúGeneral Linear Model\n(GLM): Gaussian GLM.‚Äù 8AD‚Äì4AD."
  },
  {
    "objectID": "posts/2021-08-04-gaussian-general-linear-model-glm/index.html",
    "href": "posts/2021-08-04-gaussian-general-linear-model-glm/index.html",
    "title": "General Linear Model (GLM): Gaussian GLM",
    "section": "",
    "text": "Exploratory data analysis\nThe first step in our statistical analysis should familiarising ourselves with the data. To do this, you should be performing exploratory data analysis (EDA).\n\n(i) Check the data corresponds to the experimental design\nThe first thing I usually do is check that the number of replicates per treatment is in line with the experimental design. In other words, we sampled 30 mussels per site, so let‚Äôs just check that each site has 30 mussel body mass measurements.\n\n# Start with the data frame\ndata %>%\n  # Calculate the number of replicates per 'site' \n  dplyr::count(site)\n\n  site  n\n1    A 30\n2    B 30\n\n\nAs expected, there are 30 rows of data per site. Good start!\n\n\n(ii) Visualise the data\nThe next step is to plot your data. You should be noting aspects such as:\n\nIs there a visual difference in means or the distributions of values in the different groups?\nIs there more variation in one group vs another group?\n\n\n# Start with the data frame\ndata %>% \n  ggplot(data = ., aes(x = site,\n                       y = body_mass)) +\n  geom_boxplot()\n\n\n\n\nIn this case, it looks like the median body mass (indicated by the bold black line) of mussels collected at site A is lower than than at site B). The variance (i.e.¬†spread of body mass values along the y-axis) appears to be relatively similar between the two sites.\n\n\n\nFitting the model\nFitting a GLM is relatively simple in R. All we need to do is tell it what is our response variable (the response variable is the measurement we are interested in). Here, the response variable is (body_mass). We then specify our predictor variables to the right-hand side of this weird ~ (tilde) symbol. Our predictor variables are things we have recorded that we believe could be affecting the response variable. Here, our predictor variable was site. We need to tell R where these data are stored (data), and that we want a gaussian GLM (family = gaussian()).\n\nmod1 <- glm(body_mass ~ site, \n            data = data,\n            family = gaussian(link = \"identity\"))\n\n\n\nEvaluating model fit\nBefore we look at the results from our model, we must first check whether the GLM that we fit was an appropriate choice for our data. We do that by looking at model diagnostics. Model diagnostics rely heavily on calculating and visualising residuals. For example, let‚Äôs assume we had fit a model looking at whether body length is a predictor of body mass below. The residuals are the filled black circles, indicating the deviation between our observed data value and its expected value under the given model (indicating by the unfilled circle). Simply, residuals are the observed data value minus its expected value from a model. GLM‚Äôs make assumptions about the distribution of the residuals which we are going to unpack below.\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nIn my opinion, the easiest and most informative way perform model diagnostics check in R is using the amazing package DHARMa. Below, we are going to plot two different graphs to evaluate whether our choice of model was okay.\nBefore we make any plots, we have to get DHARMa to produce residuals for us to use in our plots. We will feed these residuals into the different plotting functions.\n\nsimulated_resids <- DHARMa::simulateResiduals(fittedModel = mod1, \n                                              plot = F)\n\n\n(i) Plot #1: QQPlot\nThe QQ plot tells us whether our data conforms to the distribution we specified in the family argument in the GLM call above (remember: we said family = gaussian). If our GLM is a good fit to the data, the white triangles will fall approximately on the red 1:1 line and the KS test P-value will be greater than 0.05. The Kolmorogorov-Smirnoff test (KS test) is a formal statistical test to evaluate whether our data follow the distribution we specified in the family argument in the GLM call above.\n\nDHARMa::plotQQunif(simulated_resids)\n\n\n\n\nHere, we can see two important things. Firstly, the white triangles fall approximately along the red 1:1 line, indicating that our residuals (the white triangles) approximate the data distribution we specified in family = .... Secondly, the Kolmorogorov-Smirnoff test (KS test) provides further support that the residuals from our model were not significantly different from the the data distribution we specified in family = .... Taken together, the QQplot shows us that our residuals are approximately normally distributed. Great!\n\n\n(ii) Plot #2: Residuals vs Predicted Plot\nThis plots allows us to evaluate whether our data demonstrates heteroscedasticity. This is just a fancy way to say that the variance in the data is systematically dependent on some variable in the model. If you remember back to undergraduate statistics courses, linear models typically assume that the data displays equal variances across groups or numeric variables. When our predictor variables are categorical, such as in the current model (remember site was our predictor variable with two levels, siteA and siteB), this plot will show boxplots with each level (e.g.¬†each site) getting its own boxplot.\nIf our GLM was a good fit, we would like the boxplots to be centered between y = 0.25 to y = 0.75, with the bold black line falling approximately on the y = 0.5 line. DHARMa has recently introduced a nice function which automatically produces a formal statistical test for within-group uniformity and between-group homogeneity of variances. Ultimately, we want both of these tests to return n.s. meaning a non-significant result.\n\nDHARMa::plotResiduals(simulated_resids)\n\n\n\n\nThe plot shows us that there aren‚Äôt major concerns over unequal variances. While we would like to see the left-hand boxplox grey-shaded area range from y = 0.25 to 0.75 (it ranges from y = 0.33ish to 0.80), both the test for uniformity and unequal variances test were n.s.. In later posts, we will unpack model diagnostics in much greater detail. Stay tuned.\n\n\n\nStatistical inference\nNow to the bit of the analysis that most ecologists are most interested in (at least to appease their reviewers: assessing statistical significance and calculating p-values). Here, we perform statistical inference, which basically means we are going to evaluate ‚Äúwhich [model] coefficients are non-zero beyond a reasonable doubt, implying meaningful associations between covariates and the response?‚Äù (Tredennick et al. 2021).\nTo do this, we will use a Likelihood Ratio Test (LRT). When we only have 1 predictor variable (here: site), we can calculate p-values using type I sum-of-squares (SOS). SOS‚Äôs are just different ways that we ask R to calculate p-values.\nPLEASE DO NOT USE SUMMARY() - THIS WILL PRODUCE THE WRONG P-VALUES WHEN YOU HAVE MORE THAN 1 PREDICTOR VARIABLE\n\n# Perform LRT with type I sum-of-squares \nanova(mod1,\n      test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: gaussian, link: identity\n\nResponse: body_mass\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                    59     8481.1              \nsite  1     1498        58     6983.1 0.0004198 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, the LRT tells us that mussel body_mass was statistically significantly different between sites (\\(\\chi\\)2 = 1498, d.f. = 1, P < 0.001).\n\n\nMake a figure\nNow we are going to make a figure to summarise our findings and that you can include in your thesis or paper.\n\n# Calculate mean +- standard error\ndata %>%\n  ggplot(data = ., aes(x = site,\n                       y = body_mass,\n                       fill = site)) +\n  geom_boxplot() +\n  # Fill the boxes by site\n  scale_fill_grey(start = 0.6) +\n  # Add significance letters\n  scale_x_discrete(\"Site \",\n                   labels = c(\"A\", \"B\")) +\n  annotate(\"text\", x = 1, y = 40, label = \"a\") +\n  annotate(\"text\", x = 2, y = 60, label = \"b\") +\n  # Change axis labels\n  labs(y = \"Mussel body mass (mg)\")\n\n\n\n\n\n\n\n\n\nConclusions\nThere we go!!! You are now equipped to run your own GLM. Please let me know if there are any improvements or what topics you would like me to cover going forward.\n\n\n\n\n\nReferences\n\nTredennick, Andrew T, Giles Hooker, Stephen P Ellner, and Peter B Adler. 2021. ‚ÄúA Practical Guide to Selecting Models for Exploration, Inference, and Prediction in Ecology.‚Äù Ecology 102 (6): e03336.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{f.sutton2021,\n  author = {Guy. F. Sutton and Guy F. Sutton},\n  title = {General {Linear} {Model} {(GLM):} {Gaussian} {GLM}},\n  date = {08/04/2021},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGuy. F. Sutton, and Guy F. Sutton. 8AD‚Äì4AD. ‚ÄúGeneral Linear Model\n(GLM): Gaussian GLM.‚Äù 8AD‚Äì4AD."
  },
  {
    "objectID": "posts/2021-08-01-gaussian-glm/index.html",
    "href": "posts/2021-08-01-gaussian-glm/index.html",
    "title": "General Linear Model (GLM): Gaussian GLM",
    "section": "",
    "text": "Background\nMany scared ecologists worry about which statistical test to use to analyse their hard-earned data. This is natural. Typically, some form of linear model will be used to analyse the data, whereby the relationship between a continuous response variable (y) is modelled with respect to one or more explanatory variables (x1, x2, ‚Ä¶). A general linear model (GLM) is an umbrella term for a linear model for data (residuals) that follow a normal or Gaussian distribution (more on this later). Basically, the GLM encompasses those pesky analyses you were likely taught during undergraduate statistics courses.\n- A GLM where our x is numeric is analogous to a `linear regression`\n- A GLM where our x is categorical is analogous to a `analysis of variance (ANOVA)`\nToday, we are going to work through a simple example of a Gaussian General Linear Model (GLM) with a single categorical x variable (i.e.¬†`ANOVA`) using R.\n\n\nThe data\nLet‚Äôs consider a study where we measure the body mass of 30 mussels (body_mass) from two field sites (site). Here, we have a single categorical predictor variable (site).\n\n# Simulate data to correlate X and Y amongst groups \nset.seed(2021)             \ndata1 <- data.frame(y = rnorm(n = 30, mean = 20, sd = 10),\n                    x = rep(LETTERS[1], each = 30))\ndata2 <- data.frame(y = rnorm(n = 30, mean = 35, sd = 10),\n                    x = rep(LETTERS[2], each = 30))\ndata <- dplyr::bind_rows(data1, data2) %>%\n  dplyr::rename(body_mass = y,\n                site = x)\n\n\n\nStudy questions\nWe want to evaluate whether the mass of the mussels we collected differs between the two field sites.\nH0: Mussel body mass is not statistically different between the two sites.\nH1: Mussel body mass is statistically different between the two sites.\n\n\n\nExploratory data analysis\nThe first step in our statistical analysis should familiarising ourselves with the data. To do this, you should be performing exploratory data analysis (EDA).\n\n(i) Check the data corresponds to the experimental design\nThe first thing I usually do is check that the number of replicates per treatment is in line with the experimental design. In other words, we sampled 30 mussels per site, so let‚Äôs just check that each site has 30 mussel body mass measurements.\n\n# Start with the data frame\ndata %>%\n  # Calculate the number of replicates per 'site' \n  dplyr::count(site)\n\n  site  n\n1    A 30\n2    B 30\n\n\nAs expected, there are 30 rows of data per site. Good start!\n\n\n(ii) Visualise the data\nThe next step is to plot your data. You should be noting aspects such as:\n- Is there a visual difference in means or the distributions of values in the different groups?\n- Is there more variation in one group vs another group?\n\n# Start with the data frame\ndata %>% \n  ggplot(data = ., aes(x = site,\n                       y = body_mass)) +\n  geom_boxplot()\n\n\n\n\nIn this case, it looks like the median body mass (indicated by the bold black line) of mussels collected at site A is lower than than at site B). The variance (i.e.¬†spread of body mass values along the y-axis) appears to be relatively similar between the two sites.\n\n\n\nFitting the model\nFitting a GLM is relatively simple in `R`. All we need to do is tell it what is our response variable (the response variable is the measurement we are interested in). Here, the response variable is body_mass. We then specify our predictor variables to the right-hand side of this weird ~ (tilde) symbol. Our predictor variables are things we have recorded that we believe could be affecting the response variable. Here, our predictor variable was site. We need to tell R where these data are stored (data), and that we want a gaussian GLM (`family = gaussian()`).\n\n# Fit Gaussian model \nmod1 <- glm(\n  # Response variable \n  body_mass ~ \n    # Fixed effect\n    site, \n  data = data,\n  family = gaussian(link = \"identity\")\n  )\n\n\n\nEvaluating model fit\nBefore we look at the results from our model, we must first check whether the GLM that we fit was an appropriate choice for our data. We do that by looking at model diagnostics. Model diagnostics rely heavily on calculating and visualising residuals. For example, let‚Äôs assume we had fit a model looking at whether body length is a predictor of body mass below. The residuals are the filled black circles, indicating the deviation between our observed data value and its expected value under the given model (indicating by the unfilled circle). Simply, residuals are the observed data value minus its expected value from a model. GLM‚Äôs make assumptions about the distribution of the residuals which we are going to unpack below.\n\n\n\n\n\nIn my opinion, the easiest and most informative way perform model diagnostics check in R is using the amazing package DHARMa). Below, we are going to plot two different graphs to evaluate whether our choice of model was okay.\nBefore we make any plots, we have to get DHARMa to produce residuals for us to use in our plots. We will feed these residuals into the different plotting functions.\n\nsimulated_resids <- DHARMa::simulateResiduals(\n  fittedModel = mod1, \n  plot = F\n  )\n\n\n(i) Plot #1: QQPlot\nThe QQ plot tells us whether our data conforms to the distribution we specified in the `family` argument in the GLM call above (remember: we said `family = gaussian`). If our GLM is a good fit to the data, the white triangles will fall approximately on the red 1:1 line and the KS test P-value will be greater than 0.05. The Kolmorogorov-Smirnoff test (KS test) is a formal statistical test to evaluate whether our data follow the distribution we specified in the `family` argument in the GLM call above.\n\nDHARMa::plotQQunif(simulated_resids)\n\n\n\n\nHere, we can see two important things. Firstly, the white triangles fall approximately along the red 1:1 line, indicating that our residuals (the white triangles) approximate the data distribution we specified in `family = ‚Ä¶`. Secondly, the Kolmorogorov-Smirnoff test (KS test) provides further support that the residuals from our model were not significantly different from the the data distribution we specified in `family = ‚Ä¶`. Taken together, the QQplot shows us that our residuals are approximately normally distributed. Great!\n\n\n(ii) Plot #2: Residuals vs Predicted Plot\nThis plots allows us to evaluate whether our data demonstrates heteroscedasticity. This is just a fancy way to say that the variance in the data is systematically dependent on some variable in the model. If you remember back to undergraduate statistics courses, linear models typically assume that the data displays equal variances across groups or numeric variables. When our predictor variables are categorical, such as in the current model (remember `site` was our predictor variable with two levels, `siteA` and `siteB`), this plot will show boxplots with each level (e.g.¬†each site) getting its own boxplot.\nIf our GLM was a good fit, we would like the boxplots to be centered between y = 0.25 to y = 0.75, with the bold black line falling approximately on the y = 0.5 line. DHARMa has recently introduced a nice function which automatically produces a formal statistical test for within-group uniformity and between-group homogeneity of variances. Ultimately, we want both of these tests to return `n.s.` meaning a non-significant result.\n\nDHARMa::plotResiduals(simulated_resids)\n\n\n\n\nThe plot shows us that there aren‚Äôt major concerns over unequal variances. While we would like to see the left-hand boxplox grey-shaded area range from y = 0.25 to 0.75 (it ranges from y = 0.33 to 0.80), both the test for uniformity and unequal variances test were `n.s.`. In later posts, we will unpack model diagnostics in much greater detail. Stay tuned.\n\n\n\nStatistical inference\nNow to the bit of the analysis that most ecologists are most interested in (at least to appease their reviewers: assessing statistical significance and calculating p-values). Here, we perform statistical inference, which basically means we are going to evaluate ‚Äúwhich [model] coefficients are non-zero beyond a reasonable doubt, implying meaningful associations between covariates and the response?‚Äù\nTo do this, we will use a Likelihood Ratio Test (LRT). When we only have 1 predictor variable (here: site), we can calculate p-values using type I sum-of-squares (SOS). SOS‚Äôs are just different ways that we ask R to calculate p-values.\n**PLEASE DO NOT USE SUMMARY() - THIS WILL PRODUCE THE WRONG P-VALUES WHEN YOU HAVE MORE THAN 1 PREDICTOR VARIABLE**\n\n# Perform LRT with type I sum-of-squares \ncar::Anova(\n  mod1,\n  test = \"LR\"\n  )\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: body_mass\n     LR Chisq Df Pr(>Chisq)    \nsite   12.442  1  0.0004198 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, the LRT tells us that mussel `body_mass` was statistically significantly different between sites (ùöæ = 12.44, d.f. = 1, P < 0.001).\n\n\nMake a figure\nNow we are going to make a figure to summarise our findings and that you can include in your thesis or paper.\n\n# Make plot\ndata %>%\n  ggplot(data = ., aes(x = site,\n                       y = body_mass,\n                       fill = site)) +\n  geom_boxplot() +\n  # Fill the boxes by site\n  scale_fill_grey(start = 0.6) +\n  # Add significance letters\n  scale_x_discrete(\"Site \",\n                   labels = c(\"A\", \"B\")) +\n  annotate(\"text\", x = 1, y = 40, label = \"a\") +\n  annotate(\"text\", x = 2, y = 60, label = \"b\") +\n  # Change axis labels\n  labs(y = \"Mussel body mass (mg)\")\n\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{f.sutton2021,\n  author = {Guy. F. Sutton},\n  title = {General {Linear} {Model} {(GLM):} {Gaussian} {GLM}},\n  date = {08/04/2021},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGuy. F. Sutton. 8AD‚Äì4AD. ‚ÄúGeneral Linear Model (GLM): Gaussian\nGLM.‚Äù 8AD‚Äì4AD."
  }
]